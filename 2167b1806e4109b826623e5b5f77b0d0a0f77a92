{
  "comments": [
    {
      "key": {
        "uuid": "3b029845_1c20a4e9",
        "filename": "chrome/browser/experiments/memory_ablation_experiment.cc",
        "patchSetId": 4
      },
      "lineNbr": 99,
      "author": {
        "id": 1115906
      },
      "writtenOn": "2017-12-06T00:45:23Z",
      "side": 1,
      "message": "Hm. I wonder if zram ever attempts to do compression on multiple pages [in which compression would once again be very efficient.]. Not sure...probably not.\n\nprimiano?",
      "revId": "2167b1806e4109b826623e5b5f77b0d0a0f77a92",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "8fad9fc4_cac787a0",
        "filename": "chrome/browser/experiments/memory_ablation_experiment.cc",
        "patchSetId": 4
      },
      "lineNbr": 99,
      "author": {
        "id": 1129011
      },
      "writtenOn": "2017-12-06T00:50:50Z",
      "side": 1,
      "message": "I stamp each page with its offset specifically to make all pages different and prevent KSM from coalescing them.",
      "parentUuid": "3b029845_1c20a4e9",
      "revId": "2167b1806e4109b826623e5b5f77b0d0a0f77a92",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "5f723a43_c3faaeb6",
        "filename": "chrome/browser/experiments/memory_ablation_experiment.cc",
        "patchSetId": 4
      },
      "lineNbr": 99,
      "author": {
        "id": 1115906
      },
      "writtenOn": "2017-12-06T00:56:55Z",
      "side": 1,
      "message": "but if the zram compression allows simultaneous compression for multiple pages, then the dictionary could efficiently compress the pages [I don\u0027t htink this is the case. I think zram only ever allows compression of a single page at a time (?)].",
      "parentUuid": "8fad9fc4_cac787a0",
      "revId": "2167b1806e4109b826623e5b5f77b0d0a0f77a92",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "502b3f01_2c812bdb",
        "filename": "chrome/browser/experiments/memory_ablation_experiment.cc",
        "patchSetId": 4
      },
      "lineNbr": 99,
      "author": {
        "id": 1002871
      },
      "writtenOn": "2017-12-06T14:35:27Z",
      "side": 1,
      "message": "I am not 100% sure but I doubt that zram compressed more than one page. That would be very hard to implement. From an old LWN article [1]: \". All three zprojects use a page as the unit of compression and allocate and manage page frames to store compressed pages.\"\n\nHowever, as dskiba points out, KSM might merge page and mark them as COW before ZRAM gets there. so I guess you want pages to not be identical.\n\n\n[1] https://lwn.net/Articles/545244/",
      "parentUuid": "5f723a43_c3faaeb6",
      "revId": "2167b1806e4109b826623e5b5f77b0d0a0f77a92",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "ef1d7be4_777f84a1",
        "filename": "chrome/browser/experiments/memory_ablation_experiment.cc",
        "patchSetId": 4
      },
      "lineNbr": 99,
      "author": {
        "id": 1129011
      },
      "writtenOn": "2017-12-06T23:51:54Z",
      "side": 1,
      "message": "OK, so I dug through the code.\n\ndrivers/block/zram/zcomp_lz4.c is where zram lz4 implementation lives. It calls lz4_compress() with PAGE_SIZE, i.e. never compresses more than one page. And both branches of lz4_compress() (see lib/lz4/lz4_compress.c) always memset() lz4 context before doing anything. I.e. each page is compressed separately.",
      "parentUuid": "502b3f01_2c812bdb",
      "revId": "2167b1806e4109b826623e5b5f77b0d0a0f77a92",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    }
  ]
}