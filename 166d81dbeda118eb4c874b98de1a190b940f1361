{
  "comments": [
    {
      "key": {
        "uuid": "677597d0_5c24b330",
        "filename": "third_party/zlib/contrib/optimizations/chunkcopy.h",
        "patchSetId": 10
      },
      "lineNbr": 61,
      "author": {
        "id": 1189229
      },
      "writtenOn": "2017-11-29T01:19:31Z",
      "side": 1,
      "message": "There was a comment about paying a penalty for unaligned access. From an algorithm-level, these should support unaligned access.\n\nI\u0027m not sure if the current platforms have a terrible unaligned access penalty. We might have moved into the realm of memory-speed-bound where a small penalty won\u0027t make a difference. I don\u0027t think any of them would fault.\n\nIf we want to align things (better safe than sorry), we could easily do:\nloads -- 1-byte loads in a loop (since the loads happen less. I think this is what Mike suggested)\nstores -- something like loop unrolling setup: a switch over the offset to fill the non-aligned values and then phase shift so instead of filling \"abababab\" it could fill unaligned \"a\" and continue filling \"babababa\" (overfilling an extra \"a\" at the end).",
      "range": {
        "startLine": 61,
        "startChar": 39,
        "endLine": 61,
        "endChar": 54
      },
      "revId": "166d81dbeda118eb4c874b98de1a190b940f1361",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "a3d91f6f_4c4dd2c4",
        "filename": "third_party/zlib/contrib/optimizations/chunkcopy.h",
        "patchSetId": 10
      },
      "lineNbr": 61,
      "author": {
        "id": 1189229
      },
      "writtenOn": "2017-11-29T09:16:08Z",
      "side": 1,
      "message": "I should give an example of what I mean to be more clear. :) Something like:\n\nconstexpr size_t alignment \u003d 4;\n\n// Fill the beginning unaligned bytes\nswitch (pointer % alignment) {\n  case 3:\n    load_and_store_one_byte();\n    // fall through\n  case 2:\n    load_and_store_one_byte();\n    // fall through\n  case 1:\n    load_and_store_one_byte();\n}\n\n// Loop over aligned remainder\nwhile (more_to_do) {\n  load_and_store_four_bytes();\n}",
      "parentUuid": "677597d0_5c24b330",
      "range": {
        "startLine": 61,
        "startChar": 39,
        "endLine": 61,
        "endChar": 54
      },
      "revId": "166d81dbeda118eb4c874b98de1a190b940f1361",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "e68971dc_f82a8503",
        "filename": "third_party/zlib/contrib/optimizations/chunkcopy.h",
        "patchSetId": 10
      },
      "lineNbr": 61,
      "author": {
        "id": 1193769
      },
      "writtenOn": "2017-11-29T12:35:39Z",
      "side": 1,
      "message": "All I was suggesting is that if we don\u0027t know the particular alignment of the 2, 4, or 8 byte chunks we\u0027re loading in the other methods below, we need to make sure we\u0027re using unaligned (i.e. 1-byte aligned) load methods.  Those would be the ones ending in _u8, the ones taking a const uint8_t* argument.  For instance, all the vld1_*() methods load 8 bytes (a single d-register), but vld1_u8() loads with no alignment requirement, vld1_u16() with 2 byte, vld1_u32() with 4, vld1_u64() with 8.\n\nmemcpy() can\u0027t make any assumptions about the alignment of its src and dst buffers, and I believe __builtin_memcpy() won\u0027t do that as long as we pass it void*, char*, uint8_t*, etc.  I would expect that letting the compiler generate these 16-byte loads and stores is going to end up with single unaligned 16-byte load or store instructions, just as if we\u0027d used vld1q_u8/_mm_loadu_si128 and vst1q_u8/_mm_storeu_si128.\n\nIn our experience, both here patching zlib and elsewhere (like, me in Skia), we\u0027ve found that it\u0027s generally not worth the bother and extra code complexity to align up to 4, 8, or 16 bytes before operating at full width, but rather to just get into it.  If you\u0027re used to thinking in this sort of pattern\n\n   while (not aligned)\n      small step\n   while (big step available)\n      big step\n   while (some left over)\n      small step\n\nwe have found that we can with no speed loss or sometimes speed gain just do\n\n    while (big step available)\n        big step\n    while (some left over)\n        small step",
      "parentUuid": "a3d91f6f_4c4dd2c4",
      "range": {
        "startLine": 61,
        "startChar": 39,
        "endLine": 61,
        "endChar": 54
      },
      "revId": "166d81dbeda118eb4c874b98de1a190b940f1361",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "8bf0fb42_7a50c25f",
        "filename": "third_party/zlib/contrib/optimizations/chunkcopy.h",
        "patchSetId": 10
      },
      "lineNbr": 61,
      "author": {
        "id": 1109754
      },
      "writtenOn": "2017-12-01T12:10:57Z",
      "side": 1,
      "message": "Yes I thought your original comments about the load methods used in the NEON code too, where you remarked that there were issues with code and a compiler\u0027s freedom to assume alignment (time-bomb).\n\nI agree that memcpy() can\u0027t make any assumptions about buffer alignment, and nor can __builtin_memcpy() under the conditions you mentioned. \n\nloadchunk() loads 16 bytes into an SIMD register, and maybe does with it a single vld1q_u8 or _mm_loadu_si128.  storechunk() could use V_STORE_128 I note, but does not, preferring __builtin_memcpy() perhaps due to the reasonable expectation that the compiler will emit a single unaligned 16-byte store. \n\nAs far as the alignment patterns you have experienced: in the alder32_simd patch on Intel, we didn\u0027t care about initial alignment small step (we measured it and found no different on modern Intel CPU).  We did pre-align on the ARM port though, after noting that the ARM7/8 documentation said there would be a penalty for unaligned access:\n\n  - DDI0487B: ARMv8-A ARM Architecture Reference Manual\n  Section B2.4.3 Unaligned data access restrictions \n  Unaligned accesses typically takes a number of additional cycles to\n  complete compared to a naturally-aligned access.\n\n  - DDI0406C: ARMV7-A ARMv7-R ARM Architecture Reference Manual\n  Section A3.2.3 Unaligned data access restrictions in ARMv7 and ARMv6\n  Unaligned accesses typically take a number of additional cycles to\n  complete compared to a naturally aligned transfer. The real-time\n  implications must be analyzed carefully and key data structures might\n  need to have their alignment adjusted for optimum performance.\n  Unaligned access operations must not be used for accessing memory-\n  mapped registers in a Device or Strongly-ordered memory region.\n\n\nThis is interesting and all, but I wasn\u0027t changing the behavior of the load/storechunk in this patch :). It there was a FIXME you had in mind to add about them here, let me know.",
      "parentUuid": "e68971dc_f82a8503",
      "range": {
        "startLine": 61,
        "startChar": 39,
        "endLine": 61,
        "endChar": 54
      },
      "revId": "166d81dbeda118eb4c874b98de1a190b940f1361",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": false
    }
  ]
}