#!/usr/bin/env python
# Copyright 2016 The Chromium Authors. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

"""Script to generate chromium.gpu.json and chromium.gpu.fyi.json in
the src/testing/buildbot directory. Maintaining these files by hand is
too unwieldy.
"""

import collections
import copy
import json
import os
import string
import sys

THIS_DIR = os.path.dirname(os.path.abspath(__file__))
SRC_DIR = os.path.dirname(os.path.dirname(THIS_DIR))

CHROMIUM_GTESTS = {
  'accessibility_unittests': {},
  'app_shell_unittests': {},
  'aura_unittests': {},
  'base_unittests': {},
  'battor_agent_unittests': {},
  'blink_heap_unittests': {},
  'blink_platform_unittests': {},
  'boringssl_crypto_tests': {},
  'boringssl_ssl_tests': {},
  'browser_tests': {
    'swarming': {
      'shards': 10,
    },
  },
  'cacheinvalidation_unittests': {},
  'capture_unittests': {},
  'cast_unittests': {},
  'cc_unittests': {},
  'chrome_app_unittests': {},
  'chrome_elf_import_unittests': {},
  'chrome_elf_unittests': {},
  'chromedriver_unittests': {},
  'components_browsertests': {},
  'components_unittests': {},
  'compositor_unittests': {},
  'content_browsertests': {},
  'content_unittests': {},
  'courgette_unittests': {},
  'crashpad_tests': {},
  'crypto_unittests': {},
  'device_unittests': {},
  'display_unittests': {},
  'events_unittests': {},
  'extensions_browsertests': {},
  'extensions_unittests': {},
  'gcm_unit_tests': {},
  'gfx_unittests': {},
  'gin_unittests': {},
  'gn_unittests': {},
  'google_apis_unittests': {},
  'gpu_ipc_service_unittests': {},
  'gpu_unittests': {},
  'headless_browsertests': {},
  'headless_unittests': {},
  'install_static_unittests': {},
  'installer_util_unittests': {
    'swarming': {
      'dimension_sets': [
        {
          'integrity': 'high',
        }
      ],
    },
  },
  'interactive_ui_tests': {
    'swarming': {
      'shards': 2,
    },
  },
  'ipc_tests': {},
  'jingle_unittests': {},
  'keyboard_unittests': {},
  'latency_unittests': {},
  'libjingle_xmpp_unittests': {},
  'media_blink_unittests': {},
  'media_unittests': {},
  'message_center_unittests': {},
  'midi_unittests': {},
  'mojo_common_unittests': {},
  'mojo_public_bindings_unittests': {},
  'mojo_public_system_unittests': {},
  'mojo_system_unittests': {},
  'nacl_loader_unittests': {},
  'native_theme_unittests': {},
  'net_unittests': {},
  'pdf_unittests': {},
  'ppapi_unittests': {},
  'printing_unittests': {},
  'remoting_unittests': {},
  'sbox_integration_tests': {
    'swarming': {
      'dimension_sets': [
        {
          'integrity': 'high',
        }
      ],
    },
  },
  'sbox_unittests': {},
  'sbox_validation_tests': {},
  'service_manager_unittests': {},
  'services_unittests': {},
  'setup_unittests': {
    'swarming': {
      'dimension_sets': [
        {
          'integrity': 'high',
        }
      ],
    },
  },
  'skia_unittests': {},
  'sql_unittests': {},
  'storage_unittests': {},
  'sync_integration_tests': {},
  'ui_base_unittests': {},
  'ui_touch_selection_unittests': {},
  'unit_tests': {},
  'url_unittests': {},
  'views_unittests': {},
  'viz_unittests': {},
  'vr_common_unittests': {},
  'webkit_unit_tests': {},
  'wm_unittests': {},
  'wtf_unittests': {},
  'zucchini_unittests': {},
}

WIN7_32_BIT_GTESTS = {
  'base_unittests': {},
  'battor_agent_unittests': {},
  'browser_tests': {
    'swarming': {
      'shards': 5,
    },
  },
  'sbox_integration_tests': {
    'swarming': {
      'dimension_sets': [
        {
          'integrity': 'high',
        }
      ],
    },
  },
  'sbox_unittests': {},
  'sbox_validation_tests': {},
}

CHROMIUM_ISOLATED_SCRIPTS = {
  'content_shell_crash_test': {},
  'metrics_python_tests': {},
  'telemetry_gpu_unittests': {},
  'telemetry_perf_unittests': {
    'swarming': {
      'hard_timeout': 960,
      'shards': 12,
    },
  },
  'telemetry_unittests': {
    'swarming': {
      'shards': 2,
    },
  },
  'webkit_layout_tests': {
    'args': [
      '--debug',
      '--time-out-ms',
      '60000',
    ],
    'isolate_name': 'webkit_layout_tests_exparchive',
    'merge': {
      'args': [
        '--verbose',
      ],
      'script': '//third_party/WebKit/Tools/Scripts/merge-layout-test-results',
    },
    'swarming': {
      'shards': 10,
    }
  },
  'webkit_python_tests': {},
}


CHROMIUM_SCRIPTS = {
  'webkit_lint': {
    'script': 'webkit_lint.py',
  }
}


WATERFALLS = [
  {
    'name': 'chromium.win',
    # This is only needed to temporarily preserve order compared to
    # the handwritten JSON files to avoid confusion during code
    # review. Once they are all autogenerated, this feature can be
    # removed.
    'output_order': [
      'Win 7 Tests x64 (1)',
      'Win Builder',
      'Win10 Tests x64',
      'Win7 (32) Tests',
      'Win7 Tests (1)',
      'Win7 Tests (dbg)(1)',
      'WinMSVC64',
      'WinMSVC64 (dbg)',
    ],
    'builders': {
      'Win Builder': {
        'additional_compile_targets': [
          'pdf_fuzzers'
        ],
        'scripts': [
          {
            'name': 'check_network_annotations',
            'script': 'check_network_annotations.py'
          }
        ]
      },
      'WinMSVC64': {
        'additional_compile_targets': [
          'all',
        ],
      },
      'WinMSVC64 (dbg)': {
        'additional_compile_targets': [
          'all',
        ],
      }
    },
    'testers': {
      'Win 7 Tests x64 (1)': {
        'test_suites': {
          'gtests': CHROMIUM_GTESTS,
          'isolated_scripts': CHROMIUM_ISOLATED_SCRIPTS,
        },
      },
      'Win10 Tests x64': {
        'test_suites': {
          'gtests': CHROMIUM_GTESTS,
        },
      },
      'Win7 (32) Tests': {
        'test_suites': {
          'gtests': WIN7_32_BIT_GTESTS,
        }
      },
      'Win7 Tests (1)': {
        'test_suites': {
          'gtests': CHROMIUM_GTESTS,
          'isolated_scripts': CHROMIUM_ISOLATED_SCRIPTS,
          'scripts': CHROMIUM_SCRIPTS,
        },
      },
      'Win7 Tests (dbg)(1)': {
        'test_suites': {
          'gtests': CHROMIUM_GTESTS,
          'isolated_scripts': CHROMIUM_ISOLATED_SCRIPTS,
        },
      }
    },
  },
]


# Exceptions to the common test suites can only include removal from
# particular bots, and modifications on particular bots. By design,
# it's not possible to add one-off tests to bots. Instead they have to
# be added to one of the test suites above.
#
# The goal is to drive the number of exceptions to zero, to make all
# the bots behave similarly.
EXCEPTIONS = {
  'browser_tests': {
    'modifications': {
      'Win7 Tests (dbg)(1)': {
        'swarming': {
          'shards': 20,
        },
      },
    },
  },
  'blink_heap_unittests': {
    # Unclear why these only run on "Win7 Tests (1)".
    'remove_from': [
      'Win 7 Tests x64 (1)',
      'Win10 Tests x64',
      'Win7 Tests (dbg)(1)',
    ],
  },
  'blink_platform_unittests': {
    # Unclear why these only run on "Win7 Tests (1)".
    'remove_from': [
      'Win 7 Tests x64 (1)',
      'Win10 Tests x64',
      'Win7 Tests (dbg)(1)',
    ],
  },
  'components_browsertests': {
    'remove_from': [
      'Win7 Tests (dbg)(1)'
    ],
  },
  'content_browsertests': {
    'modifications': {
      'Win7 Tests (dbg)(1)': {
        'swarming': {
          'shards': 2,
        },
      },
    },
  },
  'content_shell_crash_test': {
    # Unclear why these only run on "Win7 Tests (1)".
    'remove_from': [
      'Win 7 Tests x64 (1)',
      'Win10 Tests x64',
      'Win7 Tests (dbg)(1)',
    ],
  },
  'device_unittests': {
    'remove_from': [
      'Win7 Tests (dbg)(1)',
    ],
  },
  'display_unittests': {
    'remove_from': [
      'Win7 Tests (dbg)(1)',
    ],
  },
  'gfx_unittests': {
    'remove_from': [
      'Win7 Tests (dbg)(1)',
    ],
  },
  'gin_unittests': {
    # Unclear why these only run on "Win7 Tests (1)".
    'remove_from': [
      'Win 7 Tests x64 (1)',
      'Win10 Tests x64',
      'Win7 Tests (dbg)(1)',
    ],
  },
  'google_apis_unittests': {
    'remove_from': [
      'Win7 Tests (dbg)(1)',
    ],
  },
  'interactive_ui_tests': {
    'modifications': {
      'Win7 Tests (dbg)(1)': {
        'swarming': {
          'shards': 4,
        },
      },
    },
  },
  'keyboard_unittests': {
    'remove_from': [
      'Win7 Tests (dbg)(1)',
    ],
  },
  'message_center_unittests': {
    'remove_from': [
      'Win7 Tests (dbg)(1)',
    ],
  },
  'metrics_python_tests': {
    'remove_from': [
      'Win7 Tests (dbg)(1)',
    ],
  },
  'service_manager_unittests': {
    'remove_from': [
      'Win7 Tests (dbg)(1)',
      'Win10 Tests x64',
    ],
  },
  'services_unittests': {
    'modifications': {
      'Win10 Tests x64': {
        'args': [
          '--test-launcher-filter-file=' +
          '../../testing/buildbot/filters/win10.services_unittests.filter'
        ],
      },
    },
  },
  'sync_integration_tests': {
    'modifications': {
      'Win7 Tests (dbg)(1)': {
        'swarming': {
          'shards': 2,
        },
      },
    },
  },
  'telemetry_perf_unittests': {
    'remove_from': [
      'Win7 Tests (dbg)(1)',
    ],
  },
  'vr_common_unittests': {
    'remove_from': [
      'Win 7 Tests x64 (1)',
      'Win10 Tests x64',
    ],
  },
  'viz_unittests': {
    'remove_from': [
      'Win10 Tests x64',
    ],
  },
  'webkit_layout_tests': {
    'remove_from': [
      'Win 7 Tests x64 (1)',
      'Win10 Tests x64',
      'Win7 Tests (1)',
    ],
    # TODO(kbr): the "Windows-7-SP1" Swarming dimension set is
    # implicit, from prefered_os_dimension in
    # tools/build/scripts/slave/recipe_modules/swarming/api.py. Ideally,
    # Swarming dimensions would be either specified for all testers on
    # the waterfall (and therefore, explicitly specified for all tests
    # in the generated JSON), or this would be removed, and the
    # implicit one used.
    'modifications': {
      'Win7 Tests (dbg)(1)': {
        'swarming': {
          'dimension_sets': [
            {
              'os': 'Windows-7-SP1',
            }
          ],
        },
      },
    },
  },
  'webkit_python_tests': {
    'remove_from': [
      'Win 7 Tests x64 (1)',
      'Win7 Tests (dbg)(1)',
    ],
  },
  'webkit_unit_tests': {
    'remove_from': [
      'Win 7 Tests x64 (1)',
      'Win10 Tests x64',
      'Win7 Tests (dbg)(1)',
    ],
  },
  'wtf_unittests': {
    # Unclear why these only run on "Win7 Tests (1)".
    'remove_from': [
      'Win 7 Tests x64 (1)',
      'Win10 Tests x64',
      'Win7 Tests (dbg)(1)',
    ],
  },
  'zucchini_unittests': {
    'remove_from': [
      'Win10 Tests x64',
    ],
  },
}


def should_run_on_tester(waterfall, tester_name, tester_config, test_name,
                         test_config):
  # TODO(kbr): until this script is merged with the GPU test generator, a few
  # arguments will be unused.
  del waterfall
  del tester_config
  del test_config
  # Currently, the only reason a test should not run on a given tester
  # is that it's in the exceptions. (Once the GPU waterfall generation
  # script is incorporated here, the rules will become more complex.)
  exception = EXCEPTIONS.get(test_name)
  if not exception:
    return True
  remove_from = exception.get('remove_from')
  if not remove_from:
    return True
  return tester_name not in remove_from


def get_test_modifications(test_name, tester_name):
  exception = EXCEPTIONS.get(test_name)
  if not exception:
    return None
  return exception.get('modifications', {}).get(tester_name)


def dictionary_merge(a, b, path=None, update=True):
  """http://stackoverflow.com/questions/7204805/
      python-dictionaries-of-dictionaries-merge
  merges b into a
  """
  if path is None:
    path = []
  for key in b:
    if key in a:
      if isinstance(a[key], dict) and isinstance(b[key], dict):
        dictionary_merge(a[key], b[key], path + [str(key)])
      elif a[key] == b[key]:
        pass # same leaf value
      elif isinstance(a[key], list) and isinstance(b[key], list):
        for idx in b[key]:
          a[key][idx] = dictionary_merge(a[key][idx], b[key][idx],
                                         path + [str(key), str(idx)],
                                         update=update)
      elif update:
        a[key] = b[key]
      else:
        raise Exception('Conflict at %s' % '.'.join(path + [str(key)]))
    else:
      a[key] = b[key]
  return a


def generate_gtest(waterfall, tester_name, tester_config, test_name,
                   test_config):
  if not should_run_on_tester(
      waterfall, tester_name, tester_config, test_name, test_config):
    return None
  result = copy.deepcopy(test_config)
  if 'test' in result:
    result['name'] = test_name
  else:
    result['test'] = test_name
  if not 'swarming' in result:
    result['swarming'] = {}
  result['swarming'].update({
    'can_use_on_swarming_builders': True,
  })
  # See if there are any exceptions that need to be merged into this
  # test's specification.
  modifications = get_test_modifications(test_name, tester_name)
  if modifications:
    result = dictionary_merge(result, modifications)
  return result


def generate_gtests(waterfall, tester_name, tester_config, test_dictionary):
  # The relative ordering of some of the tests is important to
  # minimize differences compared to the handwritten JSON files, since
  # Python's sorts are stable and there are some tests with the same
  # key (see gles2_conform_d3d9_test and similar variants). Avoid
  # losing the order by avoiding coalescing the dictionaries into one.
  gtests = []
  for test_name, test_config in sorted(test_dictionary.iteritems()):
    test = generate_gtest(waterfall, tester_name, tester_config,
                          test_name, test_config)
    if test:
      # generate_gtest may veto the test generation on this tester.
      gtests.append(test)
  return gtests


def generate_isolated_script_test(waterfall, tester_name, tester_config,
                                  test_name, test_config):
  if not should_run_on_tester(waterfall, tester_name, tester_config, test_name,
                              test_config):
    return None
  swarming = {
    'can_use_on_swarming_builders': True,
  }
  if 'swarming' in test_config:
    swarming.update(test_config['swarming'])
  result = {
    'isolate_name': test_config.get('isolate_name', test_name),
    'name': test_name,
    'swarming': swarming,
  }
  if 'args' in test_config:
    result['args'] = test_config['args']
  if 'merge' in test_config:
    result['merge'] = test_config['merge']
  # See if there are any exceptions that need to be merged into this
  # test's specification.
  modifications = get_test_modifications(test_name, tester_name)
  if modifications:
    result = dictionary_merge(result, modifications)
  return result


def generate_isolated_script_tests(waterfall, tester_name, tester_config,
                                   test_dictionary):
  isolated_scripts = []
  for test_name, test_config in sorted(test_dictionary.iteritems()):
    test = generate_isolated_script_test(waterfall,
      tester_name, tester_config, test_name, test_config)
    if test:
      isolated_scripts.append(test)
  return isolated_scripts


def generate_script_test(waterfall, tester_name, tester_config,
                         test_name, test_config):
  if not should_run_on_tester(waterfall, tester_name, tester_config, test_name,
                              test_config):
    return None
  result = {
    'name': test_name,
    'script': test_config['script']
  }
  return result


def generate_script_tests(waterfall, tester_name, tester_config,
                          test_dictionary):
  scripts = []
  for test_name, test_config in sorted(test_dictionary.iteritems()):
    test = generate_script_test(waterfall,
      tester_name, tester_config, test_name, test_config)
    if test:
      scripts.append(test)
  return scripts


def generate_waterfall_json(waterfall, filename):
  all_tests = {}
  for builder, config in waterfall.get('builders', {}).iteritems():
    all_tests[builder] = config
  for name, config in waterfall['testers'].iteritems():
    tests = {}
    test_suites = config['test_suites']
    if 'gtests' in test_suites:
      tests['gtest_tests'] = sorted(generate_gtests(waterfall, name, config,
                                                    test_suites['gtests']),
                                    key=lambda x: x['test'])
    if 'isolated_scripts' in test_suites:
      tests['isolated_scripts'] = sorted(generate_isolated_script_tests(
        waterfall, name, config, test_suites['isolated_scripts']),
                                         key=lambda x: x['name'])
    if 'scripts' in test_suites:
      tests['scripts'] = sorted(generate_script_tests(
        waterfall, name, config, test_suites['scripts']),
                                key=lambda x: x['name'])
    all_tests[name] = tests
  out_waterfall = all_tests
  if 'output_order' in waterfall:
    out_waterfall = collections.OrderedDict()
  out_waterfall['AAAAA1 AUTOGENERATED FILE DO NOT EDIT'] = {}
  out_waterfall['AAAAA2 See generate_buildbot_json.py to make changes'] = {}
  if 'output_order' in waterfall:
    for machine_name in waterfall['output_order']:
      out_waterfall[machine_name] = all_tests[machine_name]
  with open(os.path.join(SRC_DIR, 'testing', 'buildbot2', filename),
            'wb') as fp:
    # TODO(kbr): once all waterfalls are autogenerated, delete the output_order
    # lists and related code.
    json.dump(out_waterfall, fp, indent=2, separators=(',', ': '),
              sort_keys=True)
    fp.write('\n')

def main():
  for waterfall in WATERFALLS:
    generate_waterfall_json(waterfall, waterfall['name'] + '.json')
  return 0

if __name__ == "__main__":
  sys.exit(main())
