{
  "comments": [
    {
      "key": {
        "uuid": "18e3c441_998a7d6f",
        "filename": "base/task_scheduler/scheduler_worker_pool_impl.cc",
        "patchSetId": 3
      },
      "lineNbr": 446,
      "author": {
        "id": 1003325
      },
      "writtenOn": "2017-08-11T16:45:09Z",
      "side": 1,
      "message": "This was previously handled by simply tracking that last_get_worker_returned_nullptr_, and then we cleaned up the worker when last_get_worker_returned_nullptr_ \u003d\u003d true. Can we do the same here?\n\nThat has the benefit of always being bound to the worker thread.",
      "range": {
        "startLine": 444,
        "startChar": 2,
        "endLine": 446,
        "endChar": 55
      },
      "revId": "f255d44c781dfc653c67996d789106d2bb646c03",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "5f7b1032_40a75489",
        "filename": "base/task_scheduler/scheduler_worker_pool_impl.cc",
        "patchSetId": 3
      },
      "lineNbr": 446,
      "author": {
        "id": 1002897
      },
      "writtenOn": "2017-08-11T17:29:26Z",
      "side": 1,
      "message": "Currently, we have to scan the idle stack every time we call GetWork() (see calls to AddToIdleWorkersStack() and RemoveFromIdleWorkersStack()). Once the number of workers can grow dynamically, this might become a problem.\n\nWith this CL, there are 2 scenarios in which GetWork() can be called:\n- |is_active_| is true, worker is not on the idle stack -\u003e Check if there is work. If yes, return it. If no, add worker on top of the idle stack, set |is_active_| to false and return nullptr.\n- |is_active_| is false, worker is on the idle stack, sleep timeout just expired -\u003e Cleanup immediately, without checking for work.\n\nNote that GetWork() never scans the idle stack, except when cleaning up a worker. Are you saying that we could achieve that without |is_active_|, by using only |last_get_work_returned_nullptr_|?",
      "parentUuid": "18e3c441_998a7d6f",
      "range": {
        "startLine": 444,
        "startChar": 2,
        "endLine": 446,
        "endChar": 55
      },
      "revId": "f255d44c781dfc653c67996d789106d2bb646c03",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "e3f5a4f2_e147fb74",
        "filename": "base/task_scheduler/scheduler_worker_pool_impl.cc",
        "patchSetId": 3
      },
      "lineNbr": 446,
      "author": {
        "id": 1003325
      },
      "writtenOn": "2017-08-11T20:41:13Z",
      "side": 1,
      "message": "If last_get_work_returned_nullptr_ was true, we would already be on the idle stack, wouldn\u0027t we?\n\nThe subtlety in the old system was that we created all the workers at once, so it was possible for the first GetWork()\u003d\u003dnullptr to already be on the idle stack. Since we create workers on demand, this no longer needs to be the case.",
      "parentUuid": "5f7b1032_40a75489",
      "range": {
        "startLine": 444,
        "startChar": 2,
        "endLine": 446,
        "endChar": 55
      },
      "revId": "f255d44c781dfc653c67996d789106d2bb646c03",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "1a95750b_6fbe2958",
        "filename": "base/task_scheduler/scheduler_worker_pool_impl.cc",
        "patchSetId": 3
      },
      "lineNbr": 446,
      "author": {
        "id": 1212597
      },
      "writtenOn": "2017-08-11T20:59:32Z",
      "side": 1,
      "message": "\u003e If last_get_work_returned_nullptr_ was true, we would already be on the idle stack, wouldn\u0027t we?\n\nNot necessarily. Suppose GetWork() return null and then last_get_work_returned_nullptr_ became true. Then, it\u0027s possible we get woken up by WakeUpOneWorker (which would takes us off the idle workers stack). Afterwards, the worker will call GetWork() with last_get_work_returned_nullptr_ being true without being on the idle stack.",
      "parentUuid": "e3f5a4f2_e147fb74",
      "range": {
        "startLine": 444,
        "startChar": 2,
        "endLine": 446,
        "endChar": 55
      },
      "revId": "f255d44c781dfc653c67996d789106d2bb646c03",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "edb47b89_52ff4b7a",
        "filename": "base/task_scheduler/scheduler_worker_pool_impl.cc",
        "patchSetId": 3
      },
      "lineNbr": 446,
      "author": {
        "id": 1003325
      },
      "writtenOn": "2017-08-11T21:07:04Z",
      "side": 1,
      "message": "Ah, interesting. Indeed.\n\nI\u0027m wondering if we could just store this information in a hashtable that maintained the stack property, avoiding the disconnect between is_active_ and the idle workers stack.",
      "parentUuid": "1a95750b_6fbe2958",
      "range": {
        "startLine": 444,
        "startChar": 2,
        "endLine": 446,
        "endChar": 55
      },
      "revId": "f255d44c781dfc653c67996d789106d2bb646c03",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "adab2506_4305a33a",
        "filename": "base/task_scheduler/scheduler_worker_pool_impl.cc",
        "patchSetId": 3
      },
      "lineNbr": 446,
      "author": {
        "id": 1212597
      },
      "writtenOn": "2017-08-14T15:36:24Z",
      "side": 1,
      "message": "I had the same thought!\n\nBut, apparently, a hashtable isn\u0027t efficient for small numbers of elements (according to the readme in base/containers) (and we\u0027d expect having a small number of workers). Also, the recommended container that you would use instead for small number of elements is implemented as a vector. (so that Contains would be doing the same thing as our current Contains, defeating the purpose of augmenting the scheduler worker stack).",
      "parentUuid": "edb47b89_52ff4b7a",
      "range": {
        "startLine": 444,
        "startChar": 2,
        "endLine": 446,
        "endChar": 55
      },
      "revId": "f255d44c781dfc653c67996d789106d2bb646c03",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "127097dc_b742e4cd",
        "filename": "base/task_scheduler/scheduler_worker_pool_impl.cc",
        "patchSetId": 3
      },
      "lineNbr": 446,
      "author": {
        "id": 1003325
      },
      "writtenOn": "2017-08-14T16:55:04Z",
      "side": 1,
      "message": "And this is definitely true. Incidentally, this is one of the reasons why you don\u0027t see hashtables running around here. For small enough n, the O(n) std::vector outperforms hash table data structures because std::vector is contiguous. \n\nfdoray@ actually did a perf analysis on this when I last gave feedback to stay with the std::vector. The answer here might be to have the implementation switch to a hashtable at the tipping point.",
      "parentUuid": "adab2506_4305a33a",
      "range": {
        "startLine": 444,
        "startChar": 2,
        "endLine": 446,
        "endChar": 55
      },
      "revId": "f255d44c781dfc653c67996d789106d2bb646c03",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "7abad0a2_baf7ce26",
        "filename": "base/task_scheduler/scheduler_worker_pool_impl.cc",
        "patchSetId": 3
      },
      "lineNbr": 446,
      "author": {
        "id": 1212597
      },
      "writtenOn": "2017-08-15T20:18:33Z",
      "side": 1,
      "message": "I did some comparisons btwn vector, flat_map, and small_map (for when small_map gets backed by a std::map) here : https://chromium-review.googlesource.com/c/615033\n\nThough, I think we discussed in the sync that we want the |is_active_| bool.",
      "parentUuid": "127097dc_b742e4cd",
      "range": {
        "startLine": 444,
        "startChar": 2,
        "endLine": 446,
        "endChar": 55
      },
      "revId": "f255d44c781dfc653c67996d789106d2bb646c03",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "d7d63eb1_b7d90da4",
        "filename": "base/task_scheduler/scheduler_worker_pool_impl.cc",
        "patchSetId": 3
      },
      "lineNbr": 457,
      "author": {
        "id": 1002897
      },
      "writtenOn": "2017-08-11T17:29:26Z",
      "side": 1,
      "message": "No braces.",
      "revId": "f255d44c781dfc653c67996d789106d2bb646c03",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "e41bb89c_d5a40467",
        "filename": "base/task_scheduler/scheduler_worker_pool_impl.cc",
        "patchSetId": 3
      },
      "lineNbr": 457,
      "author": {
        "id": 1212597
      },
      "writtenOn": "2017-08-11T19:39:20Z",
      "side": 1,
      "message": "Done",
      "parentUuid": "d7d63eb1_b7d90da4",
      "revId": "f255d44c781dfc653c67996d789106d2bb646c03",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "be86096a_a9016d1a",
        "filename": "base/task_scheduler/scheduler_worker_pool_impl.cc",
        "patchSetId": 3
      },
      "lineNbr": 461,
      "author": {
        "id": 1002897
      },
      "writtenOn": "2017-08-11T17:29:26Z",
      "side": 1,
      "message": "This line is reached if |is_active_| is false and this histogram is recorded at line 491 just before we set |is_active_| to false. Doesn\u0027t that mean that you don\u0027t need to record it again here?",
      "revId": "f255d44c781dfc653c67996d789106d2bb646c03",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "9b12573e_7486a838",
        "filename": "base/task_scheduler/scheduler_worker_pool_impl.cc",
        "patchSetId": 3
      },
      "lineNbr": 461,
      "author": {
        "id": 1212597
      },
      "writtenOn": "2017-08-11T19:39:20Z",
      "side": 1,
      "message": "I still need to record it because I\u0027m about to do another wait.\n\nSuppose I record it @ line 491.\nThen I do a wait. I time out from the wait at call GetWork().\nI get to line 461 and I\u0027m about to return nullptr and then do another wait.\nSince the histogram records # of tasks between waits, I still need to record that I did zero tasks. (But I think you\u0027re right that it\u0027d always be zero tasks).\n\n(Also, there\u0027s a histogram unit test that fails if I don\u0027t have it here, so this seems to be the expected behavior).",
      "parentUuid": "be86096a_a9016d1a",
      "revId": "f255d44c781dfc653c67996d789106d2bb646c03",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "b1b22735_17233788",
        "filename": "base/task_scheduler/scheduler_worker_pool_impl.cc",
        "patchSetId": 3
      },
      "lineNbr": 461,
      "author": {
        "id": 1002897
      },
      "writtenOn": "2017-08-11T19:59:28Z",
      "side": 1,
      "message": "ok, you\u0027re right that the worker wakes up and waits without doing work. Add this so we know it\u0027s intended:\n\nDCHECK_EQ(0, num_tasks_since_last_wait_);",
      "parentUuid": "9b12573e_7486a838",
      "revId": "f255d44c781dfc653c67996d789106d2bb646c03",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "92bc23b7_21a9639c",
        "filename": "base/task_scheduler/scheduler_worker_pool_impl.cc",
        "patchSetId": 3
      },
      "lineNbr": 461,
      "author": {
        "id": 1212597
      },
      "writtenOn": "2017-08-11T20:05:22Z",
      "side": 1,
      "message": "Done (already added in patch set 4, but I guess I should\u0027ve mentioned it)",
      "parentUuid": "b1b22735_17233788",
      "revId": "f255d44c781dfc653c67996d789106d2bb646c03",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "e5486580_952405d9",
        "filename": "base/task_scheduler/scheduler_worker_pool_impl.cc",
        "patchSetId": 3
      },
      "lineNbr": 543,
      "author": {
        "id": 1002897
      },
      "writtenOn": "2017-08-11T17:29:26Z",
      "side": 1,
      "message": "assert lock acquired",
      "revId": "f255d44c781dfc653c67996d789106d2bb646c03",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "fea2e16d_fa84f897",
        "filename": "base/task_scheduler/scheduler_worker_pool_impl.cc",
        "patchSetId": 3
      },
      "lineNbr": 543,
      "author": {
        "id": 1212597
      },
      "writtenOn": "2017-08-11T19:39:20Z",
      "side": 1,
      "message": "Done",
      "parentUuid": "e5486580_952405d9",
      "revId": "f255d44c781dfc653c67996d789106d2bb646c03",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": false
    }
  ]
}