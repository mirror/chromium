{
  "comments": [
    {
      "key": {
        "uuid": "e09d473e_4a919788",
        "filename": "tools/perf/expectations.config",
        "patchSetId": 1
      },
      "lineNbr": 11,
      "author": {
        "id": 1128435
      },
      "writtenOn": "2017-12-11T22:43:27Z",
      "side": 1,
      "message": "Hmhh, this benchmark is supposed to be running on perf FYI waterfall only",
      "revId": "e87667508ecf883531191f57125d9c932293e850",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "71deff96_8cb56a86",
        "filename": "tools/perf/expectations.config",
        "patchSetId": 1
      },
      "lineNbr": 11,
      "author": {
        "id": 1132400
      },
      "writtenOn": "2017-12-11T22:47:16Z",
      "side": 1,
      "message": "That is where it is failing right now.\nhttps://ci.chromium.org/buildbot/chromium.perf.fyi/Mojo%20Linux%20Perf/?limit\u003d200\n\nBut since we decided to assign the path to the expectations config in the project config declared in run_benchmark and everywhere uses the same entry point, we currently cannot have it point to a separate expectations file. Maybe we will have to make some changes to how its done so that we can set different expectation files, but for now we need to disable the failing tests.",
      "parentUuid": "e09d473e_4a919788",
      "revId": "e87667508ecf883531191f57125d9c932293e850",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    }
  ]
}