{
  "comments": [
    {
      "key": {
        "uuid": "792fe3d1_95f90e2b",
        "filename": "base/task_scheduler/scheduler_worker_pool_impl.cc",
        "patchSetId": 4
      },
      "lineNbr": 284,
      "author": {
        "id": 1003325
      },
      "writtenOn": "2017-06-08T22:16:40Z",
      "side": 1,
      "message": "Instead of piping down the lock to start the threads in line, would this work to solve the race?\n\n\nstd::vector\u003cscoped_refptr\u003cSchedulerWorker\u003e\u003e local_workers;\n/* Create, Start and add workers to local_workers */\n\n{\n  AutoSchedulerLock auto_lock(idle_workers_stack_lock_);\n  workers_ \u003d std::move(local_workers);\n}\n\n/* Wake Up Workers */\n\nThere is the slight wrinkle with num_alive_workers where we could potentially wake up fewer than the expected workers at the end, but that\u0027s easy enough to fix up by checking and waking up more if the number changes.",
      "range": {
        "startLine": 276,
        "startChar": 4,
        "endLine": 284,
        "endChar": 5
      },
      "revId": "57c8500781d02982967d629392e7501c8110a936",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "d8626dfa_15480032",
        "filename": "base/task_scheduler/scheduler_worker_pool_impl.cc",
        "patchSetId": 4
      },
      "lineNbr": 284,
      "author": {
        "id": 1144260
      },
      "writtenOn": "2017-06-08T22:31:41Z",
      "side": 1,
      "message": "I think that would almost work. The concurrent access to num_wake_ups_before_start_ gives me the heebie-jeebies though (I think that\u0027s what you were saying we could fix up?). It\u0027s currently guarded by the idle_workers_stack_lock_ vs. ::WakeUpOneWorker() but if we defer the lock until swap, then we\u0027re racing mutation with that function.",
      "parentUuid": "792fe3d1_95f90e2b",
      "range": {
        "startLine": 276,
        "startChar": 4,
        "endLine": 284,
        "endChar": 5
      },
      "revId": "57c8500781d02982967d629392e7501c8110a936",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "f29b165d_fbfb0da0",
        "filename": "base/task_scheduler/scheduler_worker_pool_impl.cc",
        "patchSetId": 4
      },
      "lineNbr": 284,
      "author": {
        "id": 1003325
      },
      "writtenOn": "2017-06-08T23:16:36Z",
      "side": 1,
      "message": "Yep, that was the concurrent access I was discussing. I was thinking we could snapshot and get a lowerbound for this value so we don\u0027t have to hold the lock the entire time and then we fix up after we swap the workers into the member variable. \n\nOr maybe the fixup is unnecessary. As long as we create at least one (for StandbyThreadPolicy::ONE), it should be good to go. What\u0027s important is we wake up the right number of workers. The wakeup will create the threads on demand.\n\nThe subtlety here is that num_wake_ups_before_start_ stops getting updated once |workers_| has members. I will give you that that\u0027s super subtle.",
      "parentUuid": "d8626dfa_15480032",
      "range": {
        "startLine": 276,
        "startChar": 4,
        "endLine": 284,
        "endChar": 5
      },
      "revId": "57c8500781d02982967d629392e7501c8110a936",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "de4515e0_a8346c61",
        "filename": "base/task_scheduler/scheduler_worker_pool_impl.cc",
        "patchSetId": 4
      },
      "lineNbr": 284,
      "author": {
        "id": 1144260
      },
      "writtenOn": "2017-06-08T23:50:21Z",
      "side": 1,
      "message": "OK, I tried that (ps2), but it sometimes dchecks at https://cs.chromium.org/chromium/src/base/task_scheduler/scheduler_worker_pool_impl.cc?rcl\u003d1733d120eb79e2d8d49b18892e60ee1297f646f7\u0026l\u003d420 .\n\nI think workers_created_ is supposed to synonymous with !workers.empty(). So I\u0027m not sure that it makes sense to set it before workers_ has been swapped. But after Start() the thread proc reasonably expects to be able to assert that it\u0027s been created.",
      "parentUuid": "f29b165d_fbfb0da0",
      "range": {
        "startLine": 276,
        "startChar": 4,
        "endLine": 284,
        "endChar": 5
      },
      "revId": "57c8500781d02982967d629392e7501c8110a936",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "e3ed3d01_21d578e2",
        "filename": "base/task_scheduler/scheduler_worker_pool_impl.cc",
        "patchSetId": 4
      },
      "lineNbr": 284,
      "author": {
        "id": 1003325
      },
      "writtenOn": "2017-06-08T23:58:18Z",
      "side": 1,
      "message": "Ah, yeah. Some of the workers will expect to see themselves in workers_created_. That might be fixable enough by turning workers_created_ into a WaitableEvent and then signaling it when all is ready to go for reals.",
      "parentUuid": "de4515e0_a8346c61",
      "range": {
        "startLine": 276,
        "startChar": 4,
        "endLine": 284,
        "endChar": 5
      },
      "revId": "57c8500781d02982967d629392e7501c8110a936",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    }
  ]
}