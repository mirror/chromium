{
  "comments": [
    {
      "key": {
        "uuid": "1a3c2d4e_db4d843d",
        "filename": "base/message_loop/incoming_task_queue.cc",
        "patchSetId": 6
      },
      "lineNbr": 160,
      "author": {
        "id": 1001534
      },
      "writtenOn": "2017-09-10T17:01:22Z",
      "side": 1,
      "message": "How about instead of |accept_new_tasks_| we have an AtomicInt32.\n\nThe contract is that you must increment it at the top of this method. It being negative after the AtomicIncrement is equivalent to |accept_new_tasks_ \u003d\u003d false|. And you AtomicDecrement when done (in a ScopedClosure to which we pass a pre-bound RepeatingClosure (to avoid a Bind() call every PostTask()).\n\nIn the destructor you loop CAS 0 to std::numerics_limits\u003cAtomic32\u003e::min() before destroying |message_loop_|.\n\nThat seems simple and makes this CL a proper no-op? (can even get rid of |message_loop_lock_| I think?)",
      "revId": "42e020db8d57ca82f293fac616eb8f66ac614796",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "033dbf49_78c6f953",
        "filename": "base/message_loop/incoming_task_queue.cc",
        "patchSetId": 6
      },
      "lineNbr": 160,
      "author": {
        "id": 1003325
      },
      "writtenOn": "2017-09-11T16:52:31Z",
      "side": 1,
      "message": "I think this is unnecessary complexity for IncomingTaskQueue. If any issue were to arise, the complexity of making ScheduleWork fast should reside in MessagePump. That way any platform that has this issue only pays for it when it is an issue.",
      "parentUuid": "1a3c2d4e_db4d843d",
      "revId": "42e020db8d57ca82f293fac616eb8f66ac614796",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "a56f2b3e_1145c42a",
        "filename": "base/message_loop/incoming_task_queue.cc",
        "patchSetId": 6
      },
      "lineNbr": 160,
      "author": {
        "id": 1003325
      },
      "writtenOn": "2017-09-11T23:40:25Z",
      "side": 1,
      "message": "Updated to use the counter based approach.\nI still stand by my opinion that we should not be working around or accommodating APIs that violate calling contracts. The proper fix would be to fix the APIs.",
      "parentUuid": "033dbf49_78c6f953",
      "revId": "42e020db8d57ca82f293fac616eb8f66ac614796",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "085fd6ab_45248881",
        "filename": "base/message_loop/incoming_task_queue.cc",
        "patchSetId": 6
      },
      "lineNbr": 160,
      "author": {
        "id": 1001534
      },
      "writtenOn": "2017-09-11T23:55:53Z",
      "side": 1,
      "message": "Hmmmm this is way more complex than what I had in mind :(... I still think a CAS loop which yields is simple and gets the job done (can set a stop_accepting_tasks_ AtomicFlag to make sure it doesn\u0027t livelock).\n\nI\u0027m not saying we should go out of our way to support the old behaviour; just that we should keep the status quo and I think my proposal, same as before, is a good in between (allows removal of the broad RW-lock API while keeping current desired behaviour).\n\nAnd again, I\u0027m not saying I agree with the current behaviour either, just that I\u0027m not comfortable with changing it in a migration/removal CL which should be a no-op (nor do I think it\u0027s worth spending any time on after this CL unless it blocks something else).",
      "parentUuid": "a56f2b3e_1145c42a",
      "revId": "42e020db8d57ca82f293fac616eb8f66ac614796",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "e2001a8d_3b1bb325",
        "filename": "base/message_loop/incoming_task_queue.cc",
        "patchSetId": 6
      },
      "lineNbr": 190,
      "author": {
        "id": 1001534
      },
      "writtenOn": "2017-09-09T01:04:03Z",
      "side": 1,
      "message": "I just realized that incoming_queue_lock_ is also being held during the ScheduleWork, that\u0027s another non no-op change. I\u0027d really like this CL to be a no-op if at all possible.",
      "revId": "42e020db8d57ca82f293fac616eb8f66ac614796",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "5626287d_f835e2da",
        "filename": "base/message_loop/incoming_task_queue.cc",
        "patchSetId": 6
      },
      "lineNbr": 190,
      "author": {
        "id": 1003325
      },
      "writtenOn": "2017-09-09T17:13:29Z",
      "side": 1,
      "message": "I\u0027m a little confused. Where do you see this? Both WillDestroyCurrentMessageLoop() and PostPendingTask() only hold one lock at a time.",
      "parentUuid": "e2001a8d_3b1bb325",
      "revId": "42e020db8d57ca82f293fac616eb8f66ac614796",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "0efd85e4_ca4b400e",
        "filename": "base/message_loop/incoming_task_queue.cc",
        "patchSetId": 6
      },
      "lineNbr": 190,
      "author": {
        "id": 1001534
      },
      "writtenOn": "2017-09-10T17:01:22Z",
      "side": 1,
      "message": "Wait nvm, I got confused in re-reading this CL (I remembered ScheduleWork call was at end of a method and somehow thought it was this one...).",
      "parentUuid": "5626287d_f835e2da",
      "revId": "42e020db8d57ca82f293fac616eb8f66ac614796",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": false
    }
  ]
}